{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net: Convolutional Networks for Biomedical Image Segmentation \n",
    "### Olaf Ronneberger, Philipp Fischer, and Thomas Brox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some general notes\n",
    "* The original paper comes from back May 2015 that was the first release, so it's quite \"old\".\n",
    "* At the time it was simply the best.\n",
    "* It still is state-of-the-art at the time from some biomedical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction\n",
    "* Discussing previous state-of-the-art apporaches.\n",
    "* And how CNN outperform previous methods in visual recognition task.\n",
    "* Highlighting the need for per-pixel-classification for some visual tasks (especially for biomedical images).\n",
    "* Small number of training examples for biomedical task becuase of very specific field of expertise requiered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the later part of the introduction of this research paper, the authors discussed theory behind U-net architecture:\n",
    "\n",
    "* More elegant fully convolutional network proposal.\n",
    "* Base idea is to use downsampling path for **features extraction** followed by upsampling path for **precise localization** of these features in higher resolution layers.\n",
    "* By concatenating downsampling feature maps with corresponding upsampling layers we help successive conv layers to assemble a more precise output.\n",
    "\n",
    "(don't worry this is just the theory behind, when you see the implementation or the architecture it will make much more sense.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing for mention here before we jump into the actual architecture is the *overlap tile strategy*:\n",
    "* Input size > output size.\n",
    "* Output segmentation map only contains the pixels for which the full context is available in the output image.\n",
    "* Missing context on the edge/borders of input image is extrapolated by mirroring\n",
    "![](res/overlap-tile-strategy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test that this network was designed for, there were always very little training data available, so they had to come up with very excessive data augmentations strategy, another challenge that they were facing was working on more accurate separation of touching objects, just because when you have a lot of cells, many og them were touching each other, and it's easy for segmentation network to basically merge those cells, so they had to come up with a solution to penalize the network for doing so and to focus more on drawing those separation borders between cells. My intuition is that this idea comes after making a good error analysis, so maybe for our purpose we need to omit that part of the loss function, its looks quite specific-domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very well defined on the paper and it was very easy to reconstruct the network (as @ppisarski did)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture tells more than thousand words so we should jump straight to the image representing overall architecture\n",
    "![](https://raw.githubusercontent.com/cienciaydatos/ai-challenge-trees/master/unet/images/unet.png)\n",
    "On the down sampling path there's bassically feature extraction, as a common CNN, but instead of flattening the network in some part, they tried to preserve the spatial properties along the entire network, and then we have these kind of skip connections which seem to maintain some of the spatial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling path\n",
    "* 4 conv blocks (2 conv layers each) followed by max pooling layers 2x2 with stride 2 for downsampling\n",
    "* 5th conv block without max pooling (connection to upsampling path)\n",
    "* First conv block with 64 fiters on each con layer\n",
    "* Number of filters doubled with each consecurive conv block\n",
    "* Reduce resultion, increse depth\n",
    "* No padding (valid padding)\n",
    "* 3x3 filters with ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, Cropping2D\n",
    "from keras.layers import MaxPooling2D, Dropout, UpSampling2D, Input, concatenate\n",
    "\n",
    "def conv2d_block(inputs, filters=16):\n",
    "    c = inputs\n",
    "    for _ in range(2):\n",
    "        c = Conv2D(filters, (3,3), activation='relu', padding='valid') (c)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_20/Relu:0\", shape=(?, 28, 28, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = Input((572, 572, 1))\n",
    "\n",
    "# Downsampling path\n",
    "down_layers = [] \n",
    "filters = 64\n",
    "for _ in range(4):\n",
    "    x = conv2d_block(x, filters)\n",
    "    down_layers.append(x)\n",
    "    x = MaxPooling2D((2, 2), strides=2) (x)\n",
    "    filters *= 2 # Number of filters doubled with each layer\n",
    "\n",
    "x = conv2d_block(x, filters) # 5th conv block without max pooling\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling path\n",
    "* Symetric to the downsampling path (thus U-shape => U-net)\n",
    "* Number of filters for each consecurive conv block equals hall of the filters from previous conv block\n",
    "* Increse resolution, reduce depth (number of layers)\n",
    "* Concatenating feature maps from corresponding downsampling layers for more precise localization\n",
    "* Final layer is a 1x1 conv used to map each 64 component feature vector to the desired number of classes.\n",
    "* There are several upsampling operators, in particular they uses \"up-convolution\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def crop_shape(down, up):\n",
    "    ch = int(down[1] - up[1])\n",
    "    cw = int(down[2] - up[2])\n",
    "    ch1, ch2 = ch // 2, int(math.ceil(ch / 2))\n",
    "    cw1, cw2 = cw // 2, int(math.ceil(cw / 2))\n",
    "    \n",
    "    return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_29/truediv:0\", shape=(?, 388, 388, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for conv in reversed(down_layers): \n",
    "    filters //= 2\n",
    "    x = Conv2DTranspose(filters, (2, 2), strides=(2, 2),\n",
    "                        padding='same') (x)\n",
    "\n",
    "    ch, cw = crop_shape(conv._keras_shape, x._keras_shape)\n",
    "    conv = Cropping2D((ch, cw)) (conv)\n",
    "    \n",
    "    x = concatenate([x, conv])\n",
    "    x = conv2d_block(x, filters)\n",
    "\n",
    "output = Conv2D(2, (1, 1), activation='softmax') (x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some implementation notes\n",
    "* It was mentioned that they're using some dropout in the down sampling path but wasn't specified exactly where.\n",
    "* Presumably you can use the same padding and avoid all that crop stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Training\n",
    "* They implemented everything in caffe and is available [here](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)\n",
    "* For optimizer they uses SGD\n",
    "* To minimize the overhead and make maximum use of the GPU memory theyfavor large input tiles (large input size) over large batch size, they ended up using batch size of a sigle image, this is an historical note, single batch size is often a bad idea.\n",
    "* High momentum (0.99)\n",
    "* Loss function is a pixel wise softmax over the final feature map combined with the standard cross-entropy, they also precompute a weight map for each ground truth segmentation to kind of compensate the frequency of pixels from certain class.\n",
    "* We don't often see this in NN.\n",
    "* Good initialization of the weights is extremely important.\n",
    "* They mention that generating smooth deformations using random displacement factors was the most influential data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future task\n",
    "* Define loss fuction.\n",
    "* Standardize input pipeline.\n",
    "* Transfer learning on feature extractor.\n",
    "* Define model metrics.\n",
    "* Investigate the upsampling operator arXiv:1603.07285 is an awesome starting point.\n",
    "* Real implications of valid vs same padding.\n",
    "* Is there a way to add metadata to the model?\n",
    "* Post-processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
